{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "import os\n",
    "os.environ['WANDB_PROJECT'] = 'generalist_model'\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csrgraph as cg\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd44745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set current directory\n",
    "os.chdir(\"/cs/student/projects1/aibh/2024/cbaumgar/MSC_THESIS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f9b5d",
   "metadata": {},
   "source": [
    "**DATA GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Graph and Naming Utilities ---\n",
    "\n",
    "def get_graph(nodes):\n",
    "    \"\"\"Creates a standard grid graph with named nodes for a given list of nodes.\"\"\"\n",
    "    size = int(np.sqrt(len(nodes)))\n",
    "    if size * size != len(nodes):\n",
    "        raise ValueError(\"Number of nodes must be a perfect square.\")\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for r in range(size):\n",
    "        for c in range(size):\n",
    "            idx = r * size + c\n",
    "            # East\n",
    "            if c < size - 1: G.add_edge(nodes[idx], nodes[idx + 1], direction='EAST')\n",
    "            # West\n",
    "            if c > 0: G.add_edge(nodes[idx], nodes[idx - 1], direction='WEST')\n",
    "            # South\n",
    "            if r < size - 1: G.add_edge(nodes[idx], nodes[idx + size], direction='SOUTH')\n",
    "            # North\n",
    "            if r > 0: G.add_edge(nodes[idx], nodes[idx - size], direction='NORTH')\n",
    "    return G\n",
    "\n",
    "def generate_unique_names(count):\n",
    "    \"\"\"Generates a list of unique 2-character lowercase strings.\"\"\"\n",
    "    names = set()\n",
    "    while len(names) < count:\n",
    "        names.add(''.join(random.choices(string.ascii_lowercase, k=2)))\n",
    "    return list(names)\n",
    "\n",
    "# --- 2. Path Generation Logic ---\n",
    "\n",
    "def generate_random_walk(G, start_node, length):\n",
    "    \"\"\"Generates a path by performing a random walk from a start_node.\"\"\"\n",
    "    path = [start_node]\n",
    "    current_node = start_node\n",
    "    while len(path) < length:\n",
    "        neighbors = list(G.successors(current_node))\n",
    "        if not neighbors: break\n",
    "        next_node = random.choice(neighbors)\n",
    "        path.append(next_node)\n",
    "        current_node = next_node\n",
    "    return path\n",
    "\n",
    "def format_path_to_string(path, G):\n",
    "    \"\"\"Converts a list of nodes into a string of 'NODE DIRECTION NODE...'\"\"\"\n",
    "    if not path or len(path) < 2:\n",
    "        return path[0] if path else \"\"\n",
    "\n",
    "    path_str = \"\"\n",
    "    for i in range(len(path) - 1):\n",
    "        u, v = path[i], path[i+1]\n",
    "        try:\n",
    "            direction = G.edges[u, v]['direction']\n",
    "            path_str += f\"{u} {direction} \"\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Edge ({u}, {v}) not found in graph. Skipping segment.\")\n",
    "            continue\n",
    "    path_str += path[-1]\n",
    "    return path_str\n",
    "\n",
    "# --- 3. Core Data Generation Function ---\n",
    "\n",
    "def generate_training_example(G, config):\n",
    "    \"\"\"\n",
    "    Generates a single, complete training example (prompt, target) based on the\n",
    "    final recommended design.\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    # A. Generate the CONTEXT Block\n",
    "    context_len = random.randint(config['min_context_len'], config['max_context_len'])\n",
    "    context_start_node = random.choice(nodes)\n",
    "    context_path = generate_random_walk(G, context_start_node, context_len)\n",
    "    context_str = format_path_to_string(context_path, G)\n",
    "    \n",
    "    # B. Flip a Coin to Choose the TASK\n",
    "    if random.random() < 0.5:\n",
    "        # --- TASK 1: Pathfinding ([PATH]) ---\n",
    "        while True:\n",
    "            start_node, goal_node = random.sample(nodes, 2)\n",
    "            try:\n",
    "                shortest_path = next(nx.all_shortest_paths(G, source=start_node, target=goal_node))\n",
    "                if len(shortest_path) >= config['min_shortest_path_nodes']:\n",
    "                    break\n",
    "            except (nx.NetworkXNoPath, StopIteration):\n",
    "                continue\n",
    "        task_instruction = f\"[PATH] [START_NODE] {start_node} [GOAL_NODE] {goal_node}\"\n",
    "        target_str = format_path_to_string(shortest_path, G)\n",
    "        \n",
    "    else:\n",
    "        # --- TASK 2: Foraging ([WALK]) ---\n",
    "        start_node = random.choice(nodes)\n",
    "        walk_len = random.randint(config['min_walk_len'], config['max_walk_len'])\n",
    "        task_instruction = f\"[WALK] [START_NODE] {start_node} [LENGTH] {walk_len}\"\n",
    "        walk_path = generate_random_walk(G, start_node, walk_len)\n",
    "        target_str = format_path_to_string(walk_path, G)\n",
    "\n",
    "    # C. Assemble the final prompt and target strings\n",
    "    prompt = f\"[START] {context_str} [SEP] {task_instruction} [PLAN]\"\n",
    "    target = f\"{target_str} [END]\"\n",
    "    \n",
    "    return prompt, target\n",
    "\n",
    "# --- 4. Orchestration and File I/O ---\n",
    "\n",
    "def generate_and_save_dataset(n_examples, grid_size, output_file_path):\n",
    "    \"\"\"Orchestrates the generation of the full dataset and saves it to a file.\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        'min_context_len': 30,\n",
    "        'max_context_len': 150,\n",
    "        'min_shortest_path_nodes': 4,\n",
    "        'min_walk_len': 5,\n",
    "        'max_walk_len': 15,\n",
    "    }\n",
    "\n",
    "    print(f\"--- Generating {n_examples} examples for {output_file_path} ---\")\n",
    "    print(f\"Grid Size: {grid_size}x{grid_size}, Context Length: {config['min_context_len']}-{config['max_context_len']}\")\n",
    "    \n",
    "    dataset = []\n",
    "    num_nodes = grid_size * grid_size\n",
    "    \n",
    "    # Generate one graph environment to be used for this dataset split\n",
    "    node_names = generate_unique_names(num_nodes)\n",
    "    G = get_graph(node_names)\n",
    "    print(f\"Using graph with nodes: {node_names[:5]}...\")\n",
    "\n",
    "    for i in range(n_examples):\n",
    "        example = generate_training_example(G, config)\n",
    "        dataset.append(example)\n",
    "\n",
    "    print(f\"Writing {len(dataset)} examples to {output_file_path}...\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for prompt, target in dataset:\n",
    "            # Use a tab separator for easy parsing later\n",
    "            f.write(f\"{prompt}\\t{target}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f82045",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # These are now the total number of examples, which is more direct\n",
    "    NUM_TRAIN_EXAMPLES = 50000 \n",
    "    NUM_TEST_EXAMPLES = 5000\n",
    "    GRID_SIZE = 4\n",
    "\n",
    "    # Set a seed for reproducibility across all random operations\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # --- Setup Output Directory ---\n",
    "    output_dir = \"generalist_data\"\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Removing existing directory: {output_dir}\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    print(f\"Creating new directory: {output_dir}\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    # --- Generate Training Data ---\n",
    "    train_file_path = os.path.join(output_dir, \"train.txt\")\n",
    "    generate_and_save_dataset(\n",
    "        n_examples=NUM_TRAIN_EXAMPLES,\n",
    "        grid_size=GRID_SIZE,\n",
    "        output_file_path=train_file_path\n",
    "    )\n",
    "\n",
    "    # --- Generate Testing Data ---\n",
    "    # IMPORTANT: We use the same configuration but a different number of examples\n",
    "    # to create the test set. For a truly rigorous test set, you might use a \n",
    "    # different random seed or ensure the graph environment is different.\n",
    "    # For simplicity here, we'll just generate a smaller set.\n",
    "    test_file_path = os.path.join(output_dir, \"test.txt\")\n",
    "    generate_and_save_dataset(\n",
    "        n_examples=NUM_TEST_EXAMPLES,\n",
    "        grid_size=GRID_SIZE,\n",
    "        output_file_path=test_file_path\n",
    "    )\n",
    "\n",
    "    print(\"\\nProcess finished successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09b7f9",
   "metadata": {},
   "source": [
    "**MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model=None, base_model_name='gpt2', vocab_size=100):\n",
    "        self.base_model = base_model\n",
    "        self.base_model_name = base_model_name\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        if self.base_model is not None:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def continue_input(self, input_sequence, max_new_tokens=5, num_return_sequences=1, no_repeat_ngram_size=0,\n",
    "                       do_sample=False, temperature=0.7, num_beams=1):\n",
    "        input_ids = self.tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "        # Generate text\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd29d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(pth):\n",
    "    with open(pth, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d\n",
    "\n",
    "def is_valid_path(sequence, graphs):\n",
    "    # Split the sequence into parts\n",
    "    parts = sequence.split()\n",
    "\n",
    "    # Extract nodes and edges; nodes are at even indices, edges at odd indices\n",
    "    nodes = parts[::2]\n",
    "    edges = parts[1::2]\n",
    "\n",
    "    # Convert edges to a lowercase version for comparison (assuming all edges in graphs are lowercase)\n",
    "    edges = [edge.lower() for edge in edges]\n",
    "\n",
    "    # Iterate over each graph to check if the path exists\n",
    "    for graph in graphs:\n",
    "        path_exists = True\n",
    "        for i in range(len(nodes) - 1):\n",
    "            # Check if the current graph has the edge between the current node and the next node\n",
    "            if not graph.has_edge(nodes[i], nodes[i+1]):\n",
    "                path_exists = False\n",
    "                break\n",
    "\n",
    "        # If path exists in the current graph, return True\n",
    "        if path_exists:\n",
    "            return True\n",
    "\n",
    "    # If none of the graphs contain the path, return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cca5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **CREATING CUSTOM MODEL CONFIGURATION**\n",
    "\n",
    "from transformers import AutoConfig, GPT2Tokenizer\n",
    "\n",
    "# --- Configuration ---\n",
    "TOKENIZER_PATH = \"./generalist_tokenizer\"\n",
    "CONFIG_SAVE_PATH = \"./generalist_config\"\n",
    "\n",
    "# 1. Load the custom tokenizer to get its properties\n",
    "my_custom_tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "\n",
    "# 2. Load the base gpt2 config as a blueprint\n",
    "print(\"Loading base gpt2 config...\")\n",
    "config = AutoConfig.from_pretrained('gpt2')\n",
    "\n",
    "# 3. CRITICAL: Override the config with our custom tokenizer's properties\n",
    "print(f\"Overriding config vocab_size. Old: {config.vocab_size}, New: {my_custom_tokenizer.vocab_size}\")\n",
    "config.vocab_size = my_custom_tokenizer.vocab_size\n",
    "\n",
    "# Also update the special token IDs\n",
    "print(\"Updating special token IDs in the config...\")\n",
    "config.bos_token_id = my_custom_tokenizer.bos_token_id\n",
    "config.eos_token_id = my_custom_tokenizer.eos_token_id\n",
    "# If you have pad_token, etc., you can set them here too\n",
    "# config.pad_token_id = my_custom_tokenizer.pad_token_id\n",
    "\n",
    "# 4. Save the new, modified config to its own directory\n",
    "os.makedirs(CONFIG_SAVE_PATH, exist_ok=True)\n",
    "config.save_pretrained(CONFIG_SAVE_PATH)\n",
    "\n",
    "print(f\"Custom model config saved to: {CONFIG_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "def train_model_script(num_epochs=1,\n",
    "                       output_dir='generalist_model',\n",
    "                       lr=5e-05):\n",
    "    gc.collect()\n",
    "\n",
    "    save_steps = 500\n",
    "    save_total_limit = 30 \n",
    "    \n",
    "    # --- Define Clean Paths ---\n",
    "    data_dir = 'generalist_data' \n",
    "    tokenizer_dir = 'generalist_tokenizer'\n",
    "    config_dir = 'generalist_config'\n",
    "    \n",
    "    train_path = f'./{data_dir}/train.txt'\n",
    "    test_path = f'./{data_dir}/test.txt'\n",
    "\n",
    "    # --- THE FIX IS HERE ---\n",
    "    # 1. Construct the full command as a Python f-string\n",
    "    command = f\"\"\"\n",
    "    python3 ./run_clm.py \\\\\n",
    "        --config_name {config_dir} \\\\\n",
    "        --tokenizer_name {tokenizer_dir} \\\\\n",
    "        --train_file {train_path} \\\\\n",
    "        --validation_file {test_path} \\\\\n",
    "        --per_device_train_batch_size 1 \\\\\n",
    "        --per_device_eval_batch_size 1 \\\\\n",
    "        --do_train \\\\\n",
    "        --do_eval \\\\\n",
    "        --output_dir {output_dir} \\\\\n",
    "        --overwrite_output_dir \\\\\n",
    "        --num_train_epochs {num_epochs} \\\\\n",
    "        --save_strategy 'steps' \\\\\n",
    "        --save_steps {save_steps} \\\\\n",
    "        --save_total_limit {save_total_limit} \\\\\n",
    "        --eval_steps 2000 \\\\\n",
    "        --learning_rate {lr} \\\\\n",
    "        --report_to 'wandb'\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. Print the command to verify it's correct\n",
    "    print(\"--- Running Command ---\")\n",
    "    print(command)\n",
    "    print(\"-----------------------\")\n",
    "    \n",
    "    # 3. Execute the command using os.system\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e281b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for new cache directory \n",
    "project_cache_path = os.path.join(os.getcwd(), '.cache', 'huggingface_datasets')\n",
    "os.makedirs(project_cache_path, exist_ok=True)\n",
    "\n",
    "# Set the environment variable to tell the 'datasets' library to use this new path\n",
    "os.environ['HF_DATASETS_CACHE'] = project_cache_path\n",
    "\n",
    "print(f\"Hugging Face datasets cache is now set to: {os.environ['HF_DATASETS_CACHE']}\")\n",
    "\n",
    "wandb_path = os.path.join(os.getcwd(), 'wandb_local_runs')\n",
    "os.makedirs(wandb_path, exist_ok=True)\n",
    "# Set the environment variable to tell 'wandb' to use this new path\n",
    "os.environ['WANDB_DIR'] = wandb_path\n",
    "print(f\"Wandb local run directory set to: {os.environ['WANDB_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf generalist_model/\n",
    "!mkdir generalist_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b220a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model_script(num_epochs=1,\n",
    "                   output_dir='generalist_model',\n",
    "                   lr=5e-05)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
