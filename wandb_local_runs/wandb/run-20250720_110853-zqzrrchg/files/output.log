  0%|          | 0/65741 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-07-20 11:08:57,200 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  8%|▊         | 5000/65741 [05:33<1:00:14, 16.80it/s][INFO|trainer.py:3993] 2025-07-20 11:14:30,360 >> Saving model checkpoint to generalist_model/checkpoint-5000
{'loss': 3.996, 'grad_norm': 1.8769582509994507, 'learning_rate': 4.9620480369936573e-05, 'epoch': 0.01}
{'loss': 3.8053, 'grad_norm': 1.772592544555664, 'learning_rate': 4.9240200179492255e-05, 'epoch': 0.02}
{'loss': 3.7623, 'grad_norm': 1.685749888420105, 'learning_rate': 4.885991998904793e-05, 'epoch': 0.02}
{'loss': 3.7325, 'grad_norm': 1.7925020456314087, 'learning_rate': 4.847963979860362e-05, 'epoch': 0.03}
{'loss': 3.7082, 'grad_norm': 1.7773181200027466, 'learning_rate': 4.809935960815929e-05, 'epoch': 0.04}
{'loss': 3.6852, 'grad_norm': 1.6521590948104858, 'learning_rate': 4.771907941771497e-05, 'epoch': 0.05}
{'loss': 3.6723, 'grad_norm': 1.7519770860671997, 'learning_rate': 4.7338799227270655e-05, 'epoch': 0.05}
{'loss': 3.6611, 'grad_norm': 1.891970157623291, 'learning_rate': 4.6958519036826336e-05, 'epoch': 0.06}
{'loss': 3.6489, 'grad_norm': 1.7163820266723633, 'learning_rate': 4.657823884638202e-05, 'epoch': 0.07}
{'loss': 3.6462, 'grad_norm': 1.524249792098999, 'learning_rate': 4.61979586559377e-05, 'epoch': 0.08}
[INFO|configuration_utils.py:440] 2025-07-20 11:14:30,384 >> Configuration saved in generalist_model/checkpoint-5000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:14:30,387 >> Configuration saved in generalist_model/checkpoint-5000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:14:33,544 >> Model weights saved in generalist_model/checkpoint-5000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:14:33,563 >> tokenizer config file saved in generalist_model/checkpoint-5000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:14:33,578 >> Special tokens file saved in generalist_model/checkpoint-5000/special_tokens_map.json
[WARNING|logging.py:328] 2025-07-20 11:14:39,985 >> `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
 15%|█▌        | 10000/65741 [11:06<56:55, 16.32it/s][INFO|trainer.py:3993] 2025-07-20 11:20:03,370 >> Saving model checkpoint to generalist_model/checkpoint-10000
{'loss': 3.6382, 'grad_norm': 1.687095046043396, 'learning_rate': 4.581767846549338e-05, 'epoch': 0.08}
{'loss': 3.6328, 'grad_norm': 1.745026707649231, 'learning_rate': 4.543739827504906e-05, 'epoch': 0.09}
{'loss': 3.6293, 'grad_norm': 1.776166319847107, 'learning_rate': 4.5057118084604736e-05, 'epoch': 0.1}
{'loss': 3.6264, 'grad_norm': 1.747820496559143, 'learning_rate': 4.467683789416042e-05, 'epoch': 0.11}
{'loss': 3.6194, 'grad_norm': 1.5677406787872314, 'learning_rate': 4.42965577037161e-05, 'epoch': 0.11}
{'loss': 3.6155, 'grad_norm': 1.7376179695129395, 'learning_rate': 4.391627751327178e-05, 'epoch': 0.12}
{'loss': 3.6149, 'grad_norm': 1.9691411256790161, 'learning_rate': 4.353599732282746e-05, 'epoch': 0.13}
{'loss': 3.6135, 'grad_norm': 1.8454583883285522, 'learning_rate': 4.315571713238314e-05, 'epoch': 0.14}
{'loss': 3.6116, 'grad_norm': 1.7073657512664795, 'learning_rate': 4.2775436941938824e-05, 'epoch': 0.14}
{'loss': 3.6098, 'grad_norm': 1.8414483070373535, 'learning_rate': 4.2395156751494505e-05, 'epoch': 0.15}
[INFO|configuration_utils.py:440] 2025-07-20 11:20:03,376 >> Configuration saved in generalist_model/checkpoint-10000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:20:03,384 >> Configuration saved in generalist_model/checkpoint-10000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:20:06,564 >> Model weights saved in generalist_model/checkpoint-10000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:20:06,569 >> tokenizer config file saved in generalist_model/checkpoint-10000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:20:06,583 >> Special tokens file saved in generalist_model/checkpoint-10000/special_tokens_map.json
 23%|██▎       | 15000/65741 [16:31<50:44, 16.66it/s][INFO|trainer.py:3993] 2025-07-20 11:25:28,178 >> Saving model checkpoint to generalist_model/checkpoint-15000
{'loss': 3.6037, 'grad_norm': 1.7581678628921509, 'learning_rate': 4.201487656105018e-05, 'epoch': 0.16}
{'loss': 3.604, 'grad_norm': 1.733100414276123, 'learning_rate': 4.163459637060587e-05, 'epoch': 0.17}
{'loss': 3.5986, 'grad_norm': 1.78139328956604, 'learning_rate': 4.125431618016154e-05, 'epoch': 0.17}
{'loss': 3.5951, 'grad_norm': 1.8956111669540405, 'learning_rate': 4.0874035989717224e-05, 'epoch': 0.18}
{'loss': 3.5849, 'grad_norm': 1.6230498552322388, 'learning_rate': 4.049375579927291e-05, 'epoch': 0.19}
{'loss': 3.5752, 'grad_norm': 1.4767001867294312, 'learning_rate': 4.0113475608828586e-05, 'epoch': 0.2}
{'loss': 3.5315, 'grad_norm': 3.1044423580169678, 'learning_rate': 3.973319541838427e-05, 'epoch': 0.21}
{'loss': 3.4573, 'grad_norm': 3.15472149848938, 'learning_rate': 3.935291522793995e-05, 'epoch': 0.21}
{'loss': 3.343, 'grad_norm': 4.689924716949463, 'learning_rate': 3.897263503749563e-05, 'epoch': 0.22}
{'loss': 3.1101, 'grad_norm': 6.884273529052734, 'learning_rate': 3.859235484705131e-05, 'epoch': 0.23}
[INFO|configuration_utils.py:440] 2025-07-20 11:25:28,206 >> Configuration saved in generalist_model/checkpoint-15000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:25:28,209 >> Configuration saved in generalist_model/checkpoint-15000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:25:31,961 >> Model weights saved in generalist_model/checkpoint-15000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:25:31,975 >> tokenizer config file saved in generalist_model/checkpoint-15000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:25:31,981 >> Special tokens file saved in generalist_model/checkpoint-15000/special_tokens_map.json
 30%|███       | 20000/65741 [21:50<45:23, 16.80it/s][INFO|trainer.py:3993] 2025-07-20 11:30:47,382 >> Saving model checkpoint to generalist_model/checkpoint-20000
{'loss': 2.9288, 'grad_norm': 6.1661458015441895, 'learning_rate': 3.8212074656606986e-05, 'epoch': 0.24}
{'loss': 2.76, 'grad_norm': 6.985940933227539, 'learning_rate': 3.783179446616267e-05, 'epoch': 0.24}
{'loss': 2.6, 'grad_norm': 6.62286376953125, 'learning_rate': 3.745151427571835e-05, 'epoch': 0.25}
{'loss': 2.45, 'grad_norm': 7.462359428405762, 'learning_rate': 3.707123408527403e-05, 'epoch': 0.26}
{'loss': 2.3126, 'grad_norm': 6.730855941772461, 'learning_rate': 3.669095389482971e-05, 'epoch': 0.27}
{'loss': 2.1935, 'grad_norm': 7.566927433013916, 'learning_rate': 3.631067370438539e-05, 'epoch': 0.27}
{'loss': 2.1023, 'grad_norm': 17.674562454223633, 'learning_rate': 3.5930393513941074e-05, 'epoch': 0.28}
{'loss': 2.0441, 'grad_norm': 8.748580932617188, 'learning_rate': 3.5550113323496756e-05, 'epoch': 0.29}
{'loss': 1.9883, 'grad_norm': 33.36418151855469, 'learning_rate': 3.516983313305243e-05, 'epoch': 0.3}
{'loss': 1.9469, 'grad_norm': 8.703682899475098, 'learning_rate': 3.478955294260812e-05, 'epoch': 0.3}
[INFO|configuration_utils.py:440] 2025-07-20 11:30:47,410 >> Configuration saved in generalist_model/checkpoint-20000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:30:47,429 >> Configuration saved in generalist_model/checkpoint-20000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:30:51,159 >> Model weights saved in generalist_model/checkpoint-20000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:30:51,166 >> tokenizer config file saved in generalist_model/checkpoint-20000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:30:51,172 >> Special tokens file saved in generalist_model/checkpoint-20000/special_tokens_map.json
 38%|███▊      | 25000/65741 [27:08<40:06, 16.93it/s][INFO|trainer.py:3993] 2025-07-20 11:36:05,513 >> Saving model checkpoint to generalist_model/checkpoint-25000
{'loss': 1.8891, 'grad_norm': 20.186616897583008, 'learning_rate': 3.440927275216379e-05, 'epoch': 0.31}
{'loss': 1.8548, 'grad_norm': 35.56640625, 'learning_rate': 3.4028992561719474e-05, 'epoch': 0.32}
{'loss': 1.8134, 'grad_norm': 12.309967994689941, 'learning_rate': 3.364871237127516e-05, 'epoch': 0.33}
{'loss': 1.7769, 'grad_norm': 11.157729148864746, 'learning_rate': 3.326843218083084e-05, 'epoch': 0.33}
{'loss': 1.7725, 'grad_norm': 18.73973274230957, 'learning_rate': 3.288815199038652e-05, 'epoch': 0.34}
{'loss': 1.748, 'grad_norm': 27.00800132751465, 'learning_rate': 3.25078717999422e-05, 'epoch': 0.35}
{'loss': 1.735, 'grad_norm': 15.823967933654785, 'learning_rate': 3.212759160949788e-05, 'epoch': 0.36}
{'loss': 1.7302, 'grad_norm': 29.84221649169922, 'learning_rate': 3.174731141905356e-05, 'epoch': 0.37}
{'loss': 1.7213, 'grad_norm': 11.79668140411377, 'learning_rate': 3.136703122860924e-05, 'epoch': 0.37}
{'loss': 1.665, 'grad_norm': 27.710731506347656, 'learning_rate': 3.0986751038164925e-05, 'epoch': 0.38}
[INFO|configuration_utils.py:440] 2025-07-20 11:36:05,516 >> Configuration saved in generalist_model/checkpoint-25000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:36:05,518 >> Configuration saved in generalist_model/checkpoint-25000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:36:08,690 >> Model weights saved in generalist_model/checkpoint-25000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:36:08,693 >> tokenizer config file saved in generalist_model/checkpoint-25000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:36:08,695 >> Special tokens file saved in generalist_model/checkpoint-25000/special_tokens_map.json
 46%|████▌     | 30000/65741 [32:27<36:10, 16.47it/s][INFO|trainer.py:3993] 2025-07-20 11:41:24,513 >> Saving model checkpoint to generalist_model/checkpoint-30000
{'loss': 1.6794, 'grad_norm': 7.1273298263549805, 'learning_rate': 3.0606470847720606e-05, 'epoch': 0.39}
{'loss': 1.6479, 'grad_norm': 14.24066162109375, 'learning_rate': 3.022619065727628e-05, 'epoch': 0.4}
{'loss': 1.6208, 'grad_norm': 6.695596218109131, 'learning_rate': 2.9845910466831962e-05, 'epoch': 0.4}
{'loss': 1.6371, 'grad_norm': 20.78169059753418, 'learning_rate': 2.9465630276387647e-05, 'epoch': 0.41}
{'loss': 1.611, 'grad_norm': 14.94148063659668, 'learning_rate': 2.9085350085943325e-05, 'epoch': 0.42}
{'loss': 1.5992, 'grad_norm': 21.66556167602539, 'learning_rate': 2.8705069895499003e-05, 'epoch': 0.43}
{'loss': 1.5885, 'grad_norm': 10.315481185913086, 'learning_rate': 2.8324789705054684e-05, 'epoch': 0.43}
{'loss': 1.5692, 'grad_norm': 33.42532730102539, 'learning_rate': 2.794450951461037e-05, 'epoch': 0.44}
{'loss': 1.5541, 'grad_norm': 12.400788307189941, 'learning_rate': 2.7564229324166047e-05, 'epoch': 0.45}
{'loss': 1.5517, 'grad_norm': 13.093415260314941, 'learning_rate': 2.7183949133721725e-05, 'epoch': 0.46}
[INFO|configuration_utils.py:440] 2025-07-20 11:41:24,544 >> Configuration saved in generalist_model/checkpoint-30000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:41:24,556 >> Configuration saved in generalist_model/checkpoint-30000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:41:30,435 >> Model weights saved in generalist_model/checkpoint-30000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:41:30,441 >> tokenizer config file saved in generalist_model/checkpoint-30000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:41:30,445 >> Special tokens file saved in generalist_model/checkpoint-30000/special_tokens_map.json
 53%|█████▎    | 35000/65741 [37:56<30:48, 16.63it/s][INFO|trainer.py:3993] 2025-07-20 11:46:53,519 >> Saving model checkpoint to generalist_model/checkpoint-35000
{'loss': 1.5479, 'grad_norm': 12.999882698059082, 'learning_rate': 2.680366894327741e-05, 'epoch': 0.46}
{'loss': 1.5343, 'grad_norm': 22.327116012573242, 'learning_rate': 2.642338875283309e-05, 'epoch': 0.47}
{'loss': 1.5209, 'grad_norm': 15.852362632751465, 'learning_rate': 2.604310856238877e-05, 'epoch': 0.48}
{'loss': 1.5293, 'grad_norm': 16.779842376708984, 'learning_rate': 2.5662828371944447e-05, 'epoch': 0.49}
{'loss': 1.5099, 'grad_norm': 5.401244640350342, 'learning_rate': 2.528254818150013e-05, 'epoch': 0.49}
{'loss': 1.5071, 'grad_norm': 7.898289203643799, 'learning_rate': 2.490226799105581e-05, 'epoch': 0.5}
{'loss': 1.4998, 'grad_norm': 6.051592826843262, 'learning_rate': 2.4521987800611494e-05, 'epoch': 0.51}
{'loss': 1.4884, 'grad_norm': 5.138247013092041, 'learning_rate': 2.4141707610167172e-05, 'epoch': 0.52}
{'loss': 1.4868, 'grad_norm': 17.172597885131836, 'learning_rate': 2.3761427419722853e-05, 'epoch': 0.52}
{'loss': 1.4798, 'grad_norm': 10.725980758666992, 'learning_rate': 2.338114722927853e-05, 'epoch': 0.53}
[INFO|configuration_utils.py:440] 2025-07-20 11:46:53,522 >> Configuration saved in generalist_model/checkpoint-35000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:46:53,525 >> Configuration saved in generalist_model/checkpoint-35000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:46:56,697 >> Model weights saved in generalist_model/checkpoint-35000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:46:56,731 >> tokenizer config file saved in generalist_model/checkpoint-35000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:46:56,743 >> Special tokens file saved in generalist_model/checkpoint-35000/special_tokens_map.json
 61%|██████    | 40000/65741 [43:12<25:59, 16.51it/s][INFO|trainer.py:3993] 2025-07-20 11:52:09,896 >> Saving model checkpoint to generalist_model/checkpoint-40000
{'loss': 1.4699, 'grad_norm': 6.316917896270752, 'learning_rate': 2.3000867038834216e-05, 'epoch': 0.54}
{'loss': 1.4692, 'grad_norm': 15.36645793914795, 'learning_rate': 2.2620586848389894e-05, 'epoch': 0.55}
{'loss': 1.4649, 'grad_norm': 8.795303344726562, 'learning_rate': 2.2240306657945575e-05, 'epoch': 0.56}
{'loss': 1.4636, 'grad_norm': 9.832916259765625, 'learning_rate': 2.1860026467501257e-05, 'epoch': 0.56}
{'loss': 1.4488, 'grad_norm': 12.675858497619629, 'learning_rate': 2.1479746277056938e-05, 'epoch': 0.57}
{'loss': 1.4399, 'grad_norm': 50.081512451171875, 'learning_rate': 2.109946608661262e-05, 'epoch': 0.58}
{'loss': 1.4415, 'grad_norm': 4.895559787750244, 'learning_rate': 2.0719185896168297e-05, 'epoch': 0.59}
{'loss': 1.4306, 'grad_norm': 4.208953857421875, 'learning_rate': 2.033890570572398e-05, 'epoch': 0.59}
{'loss': 1.4431, 'grad_norm': 33.005428314208984, 'learning_rate': 1.9958625515279657e-05, 'epoch': 0.6}
{'loss': 1.4277, 'grad_norm': 15.877864837646484, 'learning_rate': 1.957834532483534e-05, 'epoch': 0.61}
[INFO|configuration_utils.py:440] 2025-07-20 11:52:09,921 >> Configuration saved in generalist_model/checkpoint-40000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:52:09,924 >> Configuration saved in generalist_model/checkpoint-40000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:52:13,149 >> Model weights saved in generalist_model/checkpoint-40000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:52:13,168 >> tokenizer config file saved in generalist_model/checkpoint-40000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:52:13,179 >> Special tokens file saved in generalist_model/checkpoint-40000/special_tokens_map.json
 68%|██████▊   | 45000/65741 [48:28<20:24, 16.94it/s][INFO|trainer.py:3993] 2025-07-20 11:57:25,655 >> Saving model checkpoint to generalist_model/checkpoint-45000
{'loss': 1.4278, 'grad_norm': 3.824738025665283, 'learning_rate': 1.919806513439102e-05, 'epoch': 0.62}
{'loss': 1.4128, 'grad_norm': 17.893943786621094, 'learning_rate': 1.88177849439467e-05, 'epoch': 0.62}
{'loss': 1.4092, 'grad_norm': 3.7081053256988525, 'learning_rate': 1.8437504753502382e-05, 'epoch': 0.63}
{'loss': 1.4092, 'grad_norm': 7.7591705322265625, 'learning_rate': 1.8057224563058063e-05, 'epoch': 0.64}
{'loss': 1.4008, 'grad_norm': 96.96086883544922, 'learning_rate': 1.7676944372613745e-05, 'epoch': 0.65}
{'loss': 1.4087, 'grad_norm': 59.2896728515625, 'learning_rate': 1.7296664182169422e-05, 'epoch': 0.65}
{'loss': 1.3995, 'grad_norm': 31.240190505981445, 'learning_rate': 1.6916383991725104e-05, 'epoch': 0.66}
{'loss': 1.3941, 'grad_norm': 9.395188331604004, 'learning_rate': 1.6536103801280782e-05, 'epoch': 0.67}
{'loss': 1.3948, 'grad_norm': 9.747925758361816, 'learning_rate': 1.6155823610836466e-05, 'epoch': 0.68}
{'loss': 1.3826, 'grad_norm': 94.79299926757812, 'learning_rate': 1.5775543420392148e-05, 'epoch': 0.68}
[INFO|configuration_utils.py:440] 2025-07-20 11:57:25,676 >> Configuration saved in generalist_model/checkpoint-45000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 11:57:25,679 >> Configuration saved in generalist_model/checkpoint-45000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 11:57:29,202 >> Model weights saved in generalist_model/checkpoint-45000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 11:57:29,205 >> tokenizer config file saved in generalist_model/checkpoint-45000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 11:57:29,206 >> Special tokens file saved in generalist_model/checkpoint-45000/special_tokens_map.json
 76%|███████▌  | 50000/65741 [53:45<15:38, 16.78it/s][INFO|trainer.py:3993] 2025-07-20 12:02:42,713 >> Saving model checkpoint to generalist_model/checkpoint-50000
{'loss': 1.3803, 'grad_norm': 47.332557678222656, 'learning_rate': 1.5395263229947826e-05, 'epoch': 0.69}
{'loss': 1.3812, 'grad_norm': 5.531946659088135, 'learning_rate': 1.5014983039503507e-05, 'epoch': 0.7}
{'loss': 1.3739, 'grad_norm': 2.473937749862671, 'learning_rate': 1.4634702849059187e-05, 'epoch': 0.71}
{'loss': 1.3629, 'grad_norm': 7.176736354827881, 'learning_rate': 1.4254422658614868e-05, 'epoch': 0.71}
{'loss': 1.3629, 'grad_norm': 6.343149662017822, 'learning_rate': 1.3874142468170548e-05, 'epoch': 0.72}
{'loss': 1.3504, 'grad_norm': 24.772249221801758, 'learning_rate': 1.3493862277726229e-05, 'epoch': 0.73}
{'loss': 1.3423, 'grad_norm': 20.13307762145996, 'learning_rate': 1.3113582087281909e-05, 'epoch': 0.74}
{'loss': 1.3386, 'grad_norm': 3.038132667541504, 'learning_rate': 1.273330189683759e-05, 'epoch': 0.75}
{'loss': 1.3382, 'grad_norm': 5.137894630432129, 'learning_rate': 1.2353021706393271e-05, 'epoch': 0.75}
{'loss': 1.3371, 'grad_norm': 3.8063759803771973, 'learning_rate': 1.1972741515948951e-05, 'epoch': 0.76}
[INFO|configuration_utils.py:440] 2025-07-20 12:02:42,722 >> Configuration saved in generalist_model/checkpoint-50000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 12:02:42,725 >> Configuration saved in generalist_model/checkpoint-50000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 12:02:45,981 >> Model weights saved in generalist_model/checkpoint-50000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 12:02:45,988 >> tokenizer config file saved in generalist_model/checkpoint-50000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 12:02:45,991 >> Special tokens file saved in generalist_model/checkpoint-50000/special_tokens_map.json
 84%|████████▎ | 55000/65741 [59:06<10:47, 16.59it/s][INFO|trainer.py:3993] 2025-07-20 12:08:03,641 >> Saving model checkpoint to generalist_model/checkpoint-55000
{'loss': 1.3209, 'grad_norm': 1.6473793983459473, 'learning_rate': 1.1592461325504632e-05, 'epoch': 0.77}
{'loss': 1.3232, 'grad_norm': 5.605213642120361, 'learning_rate': 1.1212181135060312e-05, 'epoch': 0.78}
{'loss': 1.3139, 'grad_norm': 4.2497639656066895, 'learning_rate': 1.0831900944615993e-05, 'epoch': 0.78}
{'loss': 1.3114, 'grad_norm': 2.48958158493042, 'learning_rate': 1.0451620754171675e-05, 'epoch': 0.79}
{'loss': 1.3076, 'grad_norm': 34.15354537963867, 'learning_rate': 1.0071340563727354e-05, 'epoch': 0.8}
{'loss': 1.3062, 'grad_norm': 2.4043147563934326, 'learning_rate': 9.691060373283036e-06, 'epoch': 0.81}
{'loss': 1.3026, 'grad_norm': 2.385762929916382, 'learning_rate': 9.310780182838715e-06, 'epoch': 0.81}
{'loss': 1.2986, 'grad_norm': 2.61405348777771, 'learning_rate': 8.930499992394397e-06, 'epoch': 0.82}
{'loss': 1.2944, 'grad_norm': 11.302355766296387, 'learning_rate': 8.550219801950076e-06, 'epoch': 0.83}
{'loss': 1.2942, 'grad_norm': 8.315340995788574, 'learning_rate': 8.169939611505758e-06, 'epoch': 0.84}
[INFO|configuration_utils.py:440] 2025-07-20 12:08:03,645 >> Configuration saved in generalist_model/checkpoint-55000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 12:08:03,647 >> Configuration saved in generalist_model/checkpoint-55000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 12:08:06,818 >> Model weights saved in generalist_model/checkpoint-55000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 12:08:06,821 >> tokenizer config file saved in generalist_model/checkpoint-55000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 12:08:06,822 >> Special tokens file saved in generalist_model/checkpoint-55000/special_tokens_map.json
 91%|█████████▏| 60000/65741 [1:04:19<05:46, 16.56it/s][INFO|trainer.py:3993] 2025-07-20 12:13:16,903 >> Saving model checkpoint to generalist_model/checkpoint-60000
{'loss': 1.2942, 'grad_norm': 1.8297972679138184, 'learning_rate': 7.789659421061439e-06, 'epoch': 0.84}
{'loss': 1.2911, 'grad_norm': 7.738407135009766, 'learning_rate': 7.409379230617119e-06, 'epoch': 0.85}
{'loss': 1.2887, 'grad_norm': 4.836071014404297, 'learning_rate': 7.0290990401728e-06, 'epoch': 0.86}
{'loss': 1.2907, 'grad_norm': 8.608803749084473, 'learning_rate': 6.64881884972848e-06, 'epoch': 0.87}
{'loss': 1.2832, 'grad_norm': 11.121376037597656, 'learning_rate': 6.268538659284161e-06, 'epoch': 0.87}
{'loss': 1.2806, 'grad_norm': 9.310978889465332, 'learning_rate': 5.888258468839841e-06, 'epoch': 0.88}
{'loss': 1.2826, 'grad_norm': 16.9176025390625, 'learning_rate': 5.507978278395523e-06, 'epoch': 0.89}
{'loss': 1.2814, 'grad_norm': 4.5126872062683105, 'learning_rate': 5.127698087951203e-06, 'epoch': 0.9}
{'loss': 1.2799, 'grad_norm': 2.7167625427246094, 'learning_rate': 4.747417897506883e-06, 'epoch': 0.91}
{'loss': 1.2701, 'grad_norm': 1.1646367311477661, 'learning_rate': 4.367137707062563e-06, 'epoch': 0.91}
[INFO|configuration_utils.py:440] 2025-07-20 12:13:16,914 >> Configuration saved in generalist_model/checkpoint-60000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 12:13:16,936 >> Configuration saved in generalist_model/checkpoint-60000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 12:13:22,851 >> Model weights saved in generalist_model/checkpoint-60000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 12:13:22,854 >> tokenizer config file saved in generalist_model/checkpoint-60000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 12:13:22,870 >> Special tokens file saved in generalist_model/checkpoint-60000/special_tokens_map.json
 99%|█████████▉| 65000/65741 [1:09:37<00:43, 16.97it/s][INFO|trainer.py:3993] 2025-07-20 12:18:34,272 >> Saving model checkpoint to generalist_model/checkpoint-65000
{'loss': 1.271, 'grad_norm': 2.166355609893799, 'learning_rate': 3.986857516618245e-06, 'epoch': 0.92}
{'loss': 1.2721, 'grad_norm': 1.374759316444397, 'learning_rate': 3.606577326173925e-06, 'epoch': 0.93}
{'loss': 1.2688, 'grad_norm': 4.920689105987549, 'learning_rate': 3.2262971357296056e-06, 'epoch': 0.94}
{'loss': 1.2705, 'grad_norm': 4.407708644866943, 'learning_rate': 2.8460169452852865e-06, 'epoch': 0.94}
{'loss': 1.2654, 'grad_norm': 8.61276626586914, 'learning_rate': 2.465736754840967e-06, 'epoch': 0.95}
{'loss': 1.2619, 'grad_norm': 1.8831287622451782, 'learning_rate': 2.0854565643966475e-06, 'epoch': 0.96}
{'loss': 1.2695, 'grad_norm': 6.725766181945801, 'learning_rate': 1.7051763739523282e-06, 'epoch': 0.97}
{'loss': 1.2737, 'grad_norm': 16.463436126708984, 'learning_rate': 1.3248961835080087e-06, 'epoch': 0.97}
{'loss': 1.2662, 'grad_norm': 3.379035472869873, 'learning_rate': 9.446159930636894e-07, 'epoch': 0.98}
{'loss': 1.2594, 'grad_norm': 2.761270046234131, 'learning_rate': 5.6433580261937e-07, 'epoch': 0.99}
[INFO|configuration_utils.py:440] 2025-07-20 12:18:34,277 >> Configuration saved in generalist_model/checkpoint-65000/config.json
[INFO|configuration_utils.py:891] 2025-07-20 12:18:34,289 >> Configuration saved in generalist_model/checkpoint-65000/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 12:18:37,468 >> Model weights saved in generalist_model/checkpoint-65000/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 12:18:37,471 >> tokenizer config file saved in generalist_model/checkpoint-65000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 12:18:37,485 >> Special tokens file saved in generalist_model/checkpoint-65000/special_tokens_map.json
100%|██████████| 65741/65741 [1:10:36<00:00, 16.90it/s][INFO|trainer.py:3993] 2025-07-20 12:19:33,290 >> Saving model checkpoint to generalist_model/checkpoint-65741
{'loss': 1.2631, 'grad_norm': 2.206336498260498, 'learning_rate': 1.840556121750506e-07, 'epoch': 1.0}
[INFO|configuration_utils.py:440] 2025-07-20 12:19:33,296 >> Configuration saved in generalist_model/checkpoint-65741/config.json
[INFO|configuration_utils.py:891] 2025-07-20 12:19:33,402 >> Configuration saved in generalist_model/checkpoint-65741/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 12:19:36,679 >> Model weights saved in generalist_model/checkpoint-65741/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 12:19:36,692 >> tokenizer config file saved in generalist_model/checkpoint-65741/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 12:19:36,699 >> Special tokens file saved in generalist_model/checkpoint-65741/special_tokens_map.json
100%|██████████| 65741/65741 [1:10:46<00:00, 16.90it/s][INFO|trainer.py:2676] 2025-07-20 12:19:43,811 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 65741/65741 [1:10:46<00:00, 15.48it/s]
{'train_runtime': 4251.1554, 'train_samples_per_second': 15.464, 'train_steps_per_second': 15.464, 'train_loss': 2.002555108727157, 'epoch': 1.0}
[INFO|trainer.py:3993] 2025-07-20 12:19:43,815 >> Saving model checkpoint to generalist_model
[INFO|configuration_utils.py:440] 2025-07-20 12:19:43,826 >> Configuration saved in generalist_model/config.json
[INFO|configuration_utils.py:891] 2025-07-20 12:19:43,837 >> Configuration saved in generalist_model/generation_config.json
[INFO|modeling_utils.py:3840] 2025-07-20 12:19:47,026 >> Model weights saved in generalist_model/model.safetensors
[INFO|tokenization_utils_base.py:2525] 2025-07-20 12:19:47,033 >> tokenizer config file saved in generalist_model/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-20 12:19:47,045 >> Special tokens file saved in generalist_model/special_tokens_map.json
***** train metrics *****
  epoch                    =        1.0
  total_flos               = 31995772GF
  train_loss               =     2.0026
  train_runtime            = 1:10:51.15
  train_samples            =      65741
  train_samples_per_second =     15.464
  train_steps_per_second   =     15.464
07/20/2025 12:19:47 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:4327] 2025-07-20 12:19:47,146 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-20 12:19:47,147 >>   Num examples = 1313
[INFO|trainer.py:4332] 2025-07-20 12:19:47,147 >>   Batch size = 1
100%|██████████| 1313/1313 [00:24<00:00, 52.73it/s]
***** eval metrics *****
  epoch                   =        1.0
  eval_accuracy           =     0.6037
  eval_loss               =     1.2512
  eval_runtime            = 0:00:25.19
  eval_samples            =       1313
  eval_samples_per_second =     52.121
  eval_steps_per_second   =     52.121
  perplexity              =     3.4946
[INFO|modelcard.py:450] 2025-07-20 12:20:12,343 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6037124804291844}]}
