{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3006a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set current directory\n",
    "os.chdir(\"/cs/student/projects1/aibh/2024/cbaumgar/MSC_THESIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9278c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new tokenizer from corpus: generalist_data/train.txt...\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer files (vocab.json, merges.txt) saved to: ./generalist_tokenizer\n",
      "Creating tokenizer_config.json...\n",
      "tokenizer_config.json created successfully.\n",
      "Vocabulary size: 1000\n",
      "special_tokens_map.json created successfully.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# **TOKENIZER TRAINING (Corrected with config file)**\n",
    "\n",
    "# %%\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "import json # <--- Import json library\n",
    "\n",
    "# --- Configuration ---\n",
    "TRAIN_FILE_PATH = os.path.join(\"generalist_data\", \"train.txt\") \n",
    "TOKENIZER_SAVE_PATH = \"./generalist_tokenizer\"\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# --- Training ---\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print(f\"Training new tokenizer from corpus: {TRAIN_FILE_PATH}...\")\n",
    "tokenizer.train(files=[TRAIN_FILE_PATH], vocab_size=VOCAB_SIZE, min_frequency=2,\n",
    "                special_tokens=[\"[PAD]\", \"[UNK]\", \"[EOS]\", \"[SOS]\"])\n",
    "\n",
    "os.makedirs(TOKENIZER_SAVE_PATH, exist_ok=True)\n",
    "tokenizer.save_model(TOKENIZER_SAVE_PATH)\n",
    "print(f\"Tokenizer files (vocab.json, merges.txt) saved to: {TOKENIZER_SAVE_PATH}\")\n",
    "\n",
    "# --- THE FIX: Create the tokenizer_config.json file ---\n",
    "print(\"Creating tokenizer_config.json...\")\n",
    "\n",
    "# Define the configuration. The key is to specify the class.\n",
    "# We also map our custom special tokens to the standard ones.\n",
    "tokenizer_config = {\n",
    "    \"tokenizer_class\": \"GPT2Tokenizer\",\n",
    "    \"bos_token\": \"[SOS]\",\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"model_max_length\": 1024 # This should match the model's context window\n",
    "}\n",
    "\n",
    "# Write the configuration to the file\n",
    "with open(os.path.join(TOKENIZER_SAVE_PATH, 'tokenizer_config.json'), 'w') as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "\n",
    "print(\"tokenizer_config.json created successfully.\")\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "# --- Also create a special_tokens_map.json for completeness ---\n",
    "# This helps the tokenizer know the string representation of its special tokens\n",
    "special_tokens_map = {\n",
    "    \"bos_token\": \"[SOS]\",\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"pad_token\": \"[PAD]\"\n",
    "}\n",
    "with open(os.path.join(TOKENIZER_SAVE_PATH, 'special_tokens_map.json'), 'w') as f:\n",
    "    json.dump(special_tokens_map, f, indent=2)\n",
    "\n",
    "print(\"special_tokens_map.json created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db184076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/aibh/2024/cbaumgar/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cs/student/projects1/aibh/2024/cbaumgar/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: \n",
      "'MODE Shortest START ab END cd PATH ab NORTH ef'\n",
      "\n",
      "Encoded Token IDs: \n",
      "[311, 326, 312, 467, 322, 912, 309, 467, 267, 454]\n",
      "\n",
      "Decoded Tokens (one per ID): \n",
      "['MODE', ' Shortest', ' START', ' ab', ' END', ' cd', ' PATH', ' ab', ' NORTH', ' ef']\n",
      "\n",
      "--- Verification ---\n",
      "❌ FAILURE: Tokenization did not work as expected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VERIFYING THE NEW TOKENIZER\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer we just trained. \n",
    "# NOTE: We use GPT2Tokenizer.from_pretrained() because it knows how to handle \n",
    "# the files our custom tokenizer saved, including special tokens.\n",
    "my_custom_tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_SAVE_PATH)\n",
    "\n",
    "# Let's test it on a sample sentence from our domain\n",
    "sample_sentence = \"MODE Shortest START ab END cd PATH ab NORTH ef\"\n",
    "print(f\"Original sentence: \\n'{sample_sentence}'\")\n",
    "\n",
    "# Encode the sentence\n",
    "encoded = my_custom_tokenizer.encode(sample_sentence)\n",
    "print(f\"\\nEncoded Token IDs: \\n{encoded}\")\n",
    "\n",
    "# Decode back to see the tokens\n",
    "decoded_tokens = [my_custom_tokenizer.decode([token_id]) for token_id in encoded]\n",
    "print(f\"\\nDecoded Tokens (one per ID): \\n{decoded_tokens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
