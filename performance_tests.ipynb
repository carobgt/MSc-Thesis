{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc47aa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/aibh/2024/cbaumgar/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/cs/student/projects1/aibh/2024/cbaumgar/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csrgraph as cg\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import csrgraph as cg\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from wonderwords import RandomWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596d18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode paths\n",
    "MODEL_OUTPUT_DIR = 'all_models' \n",
    "\n",
    "model_paths = {\n",
    "    \"generalist_v2\": os.path.join(os.getcwd(), MODEL_OUTPUT_DIR, 'generalist_v2' ), # trained on 2-letter nodes, EAST WEST NORTH SOUTH, shortest/foraging (l50)\n",
    "    \"generalist_v1\": os.path.join(os.getcwd(), MODEL_OUTPUT_DIR, 'generalist_v1' ), # trained on random nouns, includes directions, no repeats\n",
    "    \"foraging_v1\": os.path.join(os.getcwd(), MODEL_OUTPUT_DIR, 'foraging_v1' ), # trained on 2-letter nodes, foraging (l50, no MODE), U D L R\n",
    "    \"ellie\": os.path.join(os.getcwd(), MODEL_OUTPUT_DIR, 'ellie' ), # 2-letter nodes, foraging (l50, no MODE), EAST WEST NORTH SOUTH\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2a40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model=None, base_model_name='gpt2', vocab_size=100):\n",
    "        self.base_model = base_model\n",
    "        self.base_model_name = base_model_name\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        if self.base_model is not None:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "            # This is important for open-ended generation\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "\n",
    "    def continue_input(self, input_sequence, max_new_tokens=5, num_return_sequences=1, no_repeat_ngram_size=0,\n",
    "                       do_sample=False, temperature=0.7, num_beams=1):\n",
    "        \n",
    "        # 1. Tokenize the input to get both input_ids and an attention_mask\n",
    "        inputs = self.tokenizer(input_sequence, return_tensors='pt')\n",
    "        input_ids = inputs.input_ids\n",
    "        attention_mask = inputs.attention_mask\n",
    "\n",
    "        # 2. Prepare generation arguments\n",
    "        generation_kwargs = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"num_return_sequences\": num_return_sequences,\n",
    "            \"num_beams\": num_beams,\n",
    "            \"no_repeat_ngram_size\": no_repeat_ngram_size,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id\n",
    "        }\n",
    "\n",
    "        # 3. Only add temperature if we are doing sampling\n",
    "        if do_sample:\n",
    "            generation_kwargs['temperature'] = temperature\n",
    "\n",
    "        # Generate text using keyword arguments\n",
    "        output = self.model.generate(input_ids, **generation_kwargs)\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c3548",
   "metadata": {},
   "source": [
    "**LOOP COMPLETION TASK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea5e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(pth):\n",
    "    with open(pth, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d\n",
    "\n",
    "def is_valid_path(sequence, graphs):\n",
    "    # Split the sequence into parts\n",
    "    parts = sequence.split()\n",
    "\n",
    "    # Extract nodes and edges; nodes are at even indices, edges at odd indices\n",
    "    nodes = parts[::2]\n",
    "    edges = parts[1::2]\n",
    "\n",
    "    # Convert edges to a lowercase version for comparison (assuming all edges in graphs are lowercase)\n",
    "    edges = [edge.lower() for edge in edges]\n",
    "\n",
    "    # Iterate over each graph to check if the path exists\n",
    "    for graph in graphs:\n",
    "        path_exists = True\n",
    "        for i in range(len(nodes) - 1):\n",
    "            # Check if the current graph has the edge between the current node and the next node\n",
    "            if not graph.has_edge(nodes[i], nodes[i+1]):\n",
    "                path_exists = False\n",
    "                break\n",
    "\n",
    "        # If path exists in the current graph, return True\n",
    "        if path_exists:\n",
    "            return True\n",
    "\n",
    "    # If none of the graphs contain the path, return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f6e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For models trained on two-letter nodes, generate a random 2-letter name\n",
    "def generate_name() -> str:\n",
    "    \"\"\"Generate a random 2-letter name.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=2))\n",
    "\n",
    "# For models trained on nouns, generate a random noun\n",
    "def generate_noun():\n",
    "    \"\"\"Generates a single random noun.\"\"\"\n",
    "    r = RandomWord()\n",
    "    word = r.word(include_parts_of_speech=[\"nouns\"])\n",
    "    return word.replace(\" \", \"_\") if word else None\n",
    "\n",
    "def test_loop(model, loop_templates):\n",
    "    accuracy_scores = []  # Store accuracy scores for each template\n",
    "    results_dict = {}\n",
    "\n",
    "    for template in loop_templates:\n",
    "        template_accuracy = []  # Store accuracy for each iteration of the current template\n",
    "\n",
    "        for _ in range(100):  # Repeat for 10 versions of each template\n",
    "            # Fill the template with random nouns or 2-letter names\n",
    "            names = [generate_name() for _ in range(template.count(\"{}\") - 1)]\n",
    "            names += [names[0]]\n",
    "            filled_template = template.format(*names)\n",
    "            #print(filled_template)\n",
    "\n",
    "            # The true final item is the last name generated\n",
    "            true_final_item = names[-1]\n",
    "            input_len = len(filled_template.split())\n",
    "\n",
    "            # Use the model to predict/continue the input based on the filled template\n",
    "            # Adjust the prompt as needed for your specific model and task\n",
    "            prediction = model.continue_input(filled_template[0:-3],\n",
    "                                              max_new_tokens=5,\n",
    "                                              do_sample=False)\n",
    "            #print(prediction)\n",
    "            # Assuming the prediction is a string, extract the last word/item\n",
    "            predicted_items = prediction.strip().split()[0:input_len]\n",
    "            predicted_final_item = predicted_items[-1] if predicted_items else None\n",
    "            #print(f\"True final:{true_final_item}, predicted final: {predicted_final_item}\")\n",
    "\n",
    "            # Calculate accuracy for this iteration\n",
    "            is_correct = int(predicted_final_item == true_final_item)\n",
    "            #print(is_correct)\n",
    "            template_accuracy.append(is_correct)\n",
    "\n",
    "        # Calculate average accuracy for this template\n",
    "        accuracy_scores.extend(template_accuracy)\n",
    "        results_dict[template] = sum(template_accuracy) / len(template_accuracy)\n",
    "\n",
    "    # Calculate and return the overall average accuracy\n",
    "    overall_avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "    return overall_avg_accuracy, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43136d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_templates = [\"{} R {} L {}\",\n",
    "                  \"{} L {} R {}\",\n",
    "                  \"{} U {} D {}\",\n",
    "                  \"{} D {} U {}\",\n",
    "                  \"{} R {} D {} L {} U {}\",\n",
    "                  \"{} D {} L {} U {} R {}\",\n",
    "                  \"{} L {} U {} R {} D {}\",\n",
    "                  \"{} U {} R {} D {} L {}\",\n",
    "                  \"{} R {} R {} U {} L {} L {} D {}\",\n",
    "                  \"{} U {} U {} L {} D {} D {} R {}\"]\n",
    "\n",
    "loop_templates = [\"{} EAST {} WEST {}\",\n",
    "                  \"{} WEST {} EAST {}\",\n",
    "                  \"{} NORTH {} SOUTH {}\",\n",
    "                  \"{} SOUTH {} NORTH {}\",\n",
    "                  \"{} EAST {} SOUTH {} WEST {} NORTH {}\",\n",
    "                  \"{} SOUTH {} WEST {} NORTH {} EAST {}\",\n",
    "                  \"{} WEST {} NORTH {} EAST {} SOUTH {}\",\n",
    "                  \"{} NORTH {} EAST {} SOUTH {} WEST {}\",\n",
    "                  \"{} EAST {} EAST {} NORTH {} WEST {} WEST {} SOUTH {}\",\n",
    "                  \"{} NORTH {} NORTH {} WEST {} SOUTH {} SOUTH {} EAST {}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6508f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.803\n"
     ]
    }
   ],
   "source": [
    "model = GPT(base_model=model_paths[\"ellie\"],)\n",
    "average_accuracy, spatial_results_dict = test_loop(model, loop_templates)\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818a95c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.757\n"
     ]
    }
   ],
   "source": [
    "model = GPT(base_model=model_paths[\"foraging_v1\"],)\n",
    "average_accuracy, spatial_results_dict = test_loop(model, loop_templates)\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.21)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
